{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f1ec9e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jorgedch/data-vis/blob/main/notebooks/Data-Vis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cab569",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "# Data Visualization\n",
    "\n",
    "In this notebook you'll find real-world tasks that will challenge your data visualization skills and your creativity. The idea is not only to make use of various visualization methods, but to experience the insights that can only be obtained through them and to use them as story-telling tools to communicate your findings.\n",
    "\n",
    "Our tasks are divided in three levels of difficuty. The first one requires basic skills to explore a given dataset on a pre-defined datase, the second one asks you to implement a local solution in your machine and the third one requires to deploy the solution in a clound environment. These tasks also comprise three areas of application: <u>data-exploration</u>, <u>model-understanding</u> and <u>communication of results</u>. To be able to tackle them, they require that you understand the Machine Learning concepts, performance metrics and visualization tools explained in the following guide (Approx. time to read: 3 hours):\n",
    "\n",
    "- https://pair-code.github.io/what-if-tool/learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fcc1b",
   "metadata": {},
   "source": [
    "## Beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5d91a",
   "metadata": {},
   "source": [
    "To get started, let's take a look at an article published by the People+AI Research group of Google. In this article they talk about the _hidden bias_ contained in real-world datasets used to train Machine Learning models and how these models can hurt people.\n",
    "\n",
    "- https://pair.withgoogle.com/explorables/hidden-bias\n",
    "\n",
    "Can you think of another example where hidden bias in datasets can hurt people? Other times, hidden bias don't hurt people but simply make models perform worse in the production environment than in the training environment. What could be an example of this other situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503bfb1",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0bf6a0",
   "metadata": {},
   "source": [
    "Next, we make use of the _What-If_ tool developed by the same group. This time you'll interact with multiple classification models to predict the yearly income of a person based on their census information. Explore the different features available and how each of them affects the predictions made by the models.\n",
    "\n",
    "- https://pair-code.github.io/what-if-tool/demos/uci.html\n",
    "\n",
    "Here are a couple of questions concerning _data-exploration_ and _model-understanding_:\n",
    "- Take a look at the Features tab.\n",
    "  - Are the numerical features uniformly distributed?\n",
    "  - Which features are multimodal?\n",
    "  - Are the categorical features balanced?\n",
    "  - How would you ensure different types of features are not biased towards some values in your training dataset?\n",
    "- Which features help the models to make fairer predictions?\n",
    "- Which features have the most hidden bias that can hurt people?\n",
    "- Which model makes the most accurate predictions?\n",
    "- Which model has the best F1 score?\n",
    "- Based on two features that you choose, which model makes fairer predictions? Which features did you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352657d",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df2310",
   "metadata": {},
   "source": [
    "## Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cba6c",
   "metadata": {},
   "source": [
    "Now let's take a look at the code behind the online dashboards and customize it to our needs. The following code is adapted from a [PAIR](https://colab.research.google.com/github/pair-code/what-if-tool/blob/master/WIT_Model_Comparison.ipynb) tutorial and allows you to change the model's hyper-parameters and the data used for training and visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03eab1",
   "metadata": {},
   "source": [
    "Firstly, examine what are the hardware specs available for computation. To make sure that you have access to a GPU when running this notebook inside a Colab environment, go to _Edit ➞ Notebook settings ➞ Hardware accelerator_ and select _GPU_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08452ac",
   "metadata": {},
   "source": [
    "Install the What-If Tool widget if running in colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  !pip install --upgrade witwidget\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad874f52",
   "metadata": {},
   "source": [
    "Import all the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard:\n",
    "import functools\n",
    "\n",
    "# Pip:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from witwidget.notebook.visualization import WitConfigBuilder, WitWidget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2128fc",
   "metadata": {},
   "source": [
    "Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tf feature spec from the dataframe and columns specified.\n",
    "def create_feature_spec(df, columns=None):\n",
    "    feature_spec = {}\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for f in columns:\n",
    "        if df[f].dtype is np.dtype(np.int64):\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        elif df[f].dtype is np.dtype(np.float64):\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
    "        else:\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
    "    return feature_spec\n",
    "\n",
    "# Creates simple numeric and categorical feature columns from a feature spec\n",
    "# and a list of columns from that spec to use.\n",
    "#\n",
    "# NOTE: Models might perform better with some feature engineering such as bucketed\n",
    "# numeric columns and hash-bucket/embedding columns for categorical features.\n",
    "def create_feature_columns(columns, feature_spec):\n",
    "    ret = []\n",
    "    for col in columns:\n",
    "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
    "            ret.append(tf.feature_column.numeric_column(col))\n",
    "        else:\n",
    "            ret.append(tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
    "    return ret\n",
    "\n",
    "# An input function for providing input to a model from tf.Examples\n",
    "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                       num_epochs=None, \n",
    "                       batch_size=64):\n",
    "    def ex_generator():\n",
    "        for i in range(len(examples)):\n",
    "            yield examples[i].SerializeToString()\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    return dataset\n",
    "\n",
    "# Parses Tf.Example protos into features for the input function.\n",
    "def parse_tf_example(example_proto, label, feature_spec):\n",
    "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
    "    target = parsed_features.pop(label)\n",
    "    return parsed_features, target\n",
    "\n",
    "# Converts a dataframe into a list of tf.Example protos.\n",
    "def df_to_examples(df, columns=None):\n",
    "    examples = []\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        example = tf.train.Example()\n",
    "        for col in columns:\n",
    "            if df[col].dtype is np.dtype(np.int64):\n",
    "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "            elif df[col].dtype is np.dtype(np.float64):\n",
    "                example.features.feature[col].float_list.value.append(row[col])\n",
    "            elif row[col] == row[col]:\n",
    "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
    "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
    "def make_label_column_numeric(df, label_column, test):\n",
    "    df[label_column] = np.where(test(df[label_column]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6dea6b",
   "metadata": {},
   "source": [
    "Read training dataset from CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the CSV containing the dataset to train on.\n",
    "csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "\n",
    "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
    "# the column names, then set this to None.\n",
    "csv_columns = [\n",
    "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
    "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
    "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
    "\n",
    "# Read the dataset from the provided CSV and print out information about it.\n",
    "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1af43e",
   "metadata": {},
   "source": [
    "Specify input columns and column to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96750db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the column in the dataset you wish for the model to predict\n",
    "label_column = 'Over-50K'\n",
    "\n",
    "# Make the label column numeric (0 and 1), for use in our model.\n",
    "# In this case, examples with a target value of '>50K' are considered to be in\n",
    "# the '1' (positive) class and all other examples are considered to be in the\n",
    "# '0' (negative) class.\n",
    "make_label_column_numeric(df, label_column, lambda val: val == '>50K')\n",
    "\n",
    "# Set list of all columns from the dataset we will use for model input.\n",
    "input_features = [\n",
    "  'Age', 'Workclass', 'Education', 'Marital-Status', 'Occupation',\n",
    "  'Relationship', 'Race', 'Sex', 'Capital-Gain', 'Capital-Loss',\n",
    "  'Hours-per-week', 'Country']\n",
    "\n",
    "# Create a list containing all input features and the label column\n",
    "features_and_labels = input_features + [label_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1d13f",
   "metadata": {},
   "source": [
    "Convert dataset to tf.Example protos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = df_to_examples(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee85783",
   "metadata": {},
   "source": [
    "Create a feature spec and input function for both classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = create_feature_spec(df, features_and_labels)\n",
    "train_inpf = functools.partial(tfexamples_input_fn, examples, feature_spec, label_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a960ab",
   "metadata": {},
   "source": [
    "Create and train the **linear** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ce97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "# Define and train the classifier\n",
    "classifier = tf.estimator.LinearClassifier(\n",
    "    feature_columns=create_feature_columns(input_features, feature_spec)\n",
    ")\n",
    "classifier.train(train_inpf, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5719a5e",
   "metadata": {},
   "source": [
    "Create and train the **DNN** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ee1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_2 = 1000\n",
    "\n",
    "# Define and train the classifier\n",
    "classifier2 = tf.estimator.DNNClassifier(\n",
    "    feature_columns=create_feature_columns(input_features, feature_spec),\n",
    "    hidden_units=[128, 64, 32]\n",
    ")\n",
    "classifier2.train(train_inpf, steps=num_steps_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc04a30",
   "metadata": {},
   "source": [
    "Invoke What-If Tool for test data and the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7300757",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 1000\n",
    "tool_height_in_px = 1000\n",
    "\n",
    "# Load up the test dataset\n",
    "test_csv_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_df = pd.read_csv(test_csv_path, names=csv_columns, skipinitialspace=True, skiprows=1)\n",
    "make_label_column_numeric(test_df, label_column, lambda val: val == '>50K.')\n",
    "test_examples = df_to_examples(test_df[0:num_datapoints])\n",
    "\n",
    "# Setup the tool with the test examples and the trained classifier\n",
    "config_builder = WitConfigBuilder(test_examples[0:num_datapoints])\n",
    "config_builder = config_builder.set_estimator_and_feature_spec(classifier, feature_spec)\n",
    "config_builder = config_builder.set_compare_estimator_and_feature_spec(classifier2, feature_spec)\n",
    "config_builder = config_builder.set_label_vocab(['Under 50K', 'Over 50K'])\n",
    "\n",
    "WitWidget(config_builder, height=tool_height_in_px)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be785d0",
   "metadata": {},
   "source": [
    "Make the following modifications and answer the corresponding questions:\n",
    "\n",
    "- Organize datapoints by setting X-axis scatter to \"inference score 1\" and Y-axis scatter to \"inference score 2\" to see how each datapoint differs in score between the linear model (1) and DNN model (2). Points off the diagonal have differences in results between the two models.\n",
    "  - Are there patterns of which datapoints don't agree between the two models?\n",
    "  - If you set the ground truth feature dropdown in the \"Performance + Fairness\" tab to \"Over-50K\", then you can color or bin the datapoints by \"inference correct 1\" or \"inference correct 2\". Are there patterns of which datapoints are incorrect for model 1? For model 2?\n",
    "\n",
    "- Explore performance of the two models through the confusion matrices in the \"Performance + Fairness\" tab. Which model is best? Train either model for longer and see if you can change this. Are the rates of errors (false positives and false negatives) that the two models make different?\n",
    "  - Click the \"optimize threshold\" button to set the optimal positive classification threshold for each model based on the current cost ratio of 1. How do those thresholds and the resulting confusion matrices differ?\n",
    "    - Change the cost ratio and optimize the threshold again. How does the threshold and performance change on the two models?\n",
    "  - Slice the dataset by features, such as \"sex\" or \"race\". Does either model have more-equal performance between slices?\n",
    "    - Use the threshold optimization buttons to set optimal thresholds based on the different fairness constraints. How does performance between slices differ between the two models. Does one require larger differences in threshold values per slice to achieve the desired constraint?\n",
    "\n",
    "- Looking at the create_feature_columns function in the \"Define helper methods\" cell, categorical features use one-hot encodings in the model. Perhaps change a many-valued categorical feature, such as education to use an embedding layer. Does anything change in the model behavior (can look through partial dependence plots as one way to investigate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda321c",
   "metadata": {},
   "source": [
    "_Your answers here..._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26159323",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86d286",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WIT Model Comparison",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
